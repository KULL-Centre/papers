{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <b><font color='#009e74'>Rapid protein stability prediction using deep learning representations </font></b>\n",
        "\n",
        "Preprint pipeline version for predicting protein variants **thermodynamic stability changes** ($\\Delta \\Delta G$) using a deep learning representation. The program, using as input a protein structure (uploaded as PDB) returns stability predictions ($\\Delta \\Delta G$ in kcal/mol) for each variant at each position of the query protein.\n",
        "More details can be found in: **Blaabjerg et al.:** [\"Rapid protein stability prediction using deep learning representations\"](https://www.biorxiv.org/content/10.1101/2022.07.14.500157v1). Source code is available on the project [Github](https://github.com/KULL-Centre/papers/tree/main/2022/ML-ddG-Blaabjerg-et-al) page.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q8szWTVF-dXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  <b><font color='#009e74'> Reminders and Important informations:</font></b>\n",
        "- This notebook  <b><font color='#d55c00'>must</font></b> be run in a Colab GPU session (go to page menu: `Runtime`->  `Change runtime type` -> select `GPU` and confirm\n",
        "- Run <b><font color='#d55c00'>ONE</font></b> cell at a time, <b><font color='#d55c00'>DON'T USE</font></b> the `Runtime`->  `Run all` function as condacolab installation requires a restart of the kernel.\n",
        "- Cells labelled <b><font color='#56b4e9'>PRELIMINARY OPERATIONS </font></b>  must be run <b><font color='#d55c00'>ONE</font></b> at a time and <b><font color='#d55c00'>ONCE</font></b> at the start and skipped for new predictions.\n",
        "- Cells named as  <b><font color='#56b4e9'>PRELIMINARY OPERATIONS </font></b> have to be run <b><font color='#d55c00'>ONE BY ONE</font></b>  and <font color='#d55c00'>ONCE only at the start</font></b>  and  skipped for new predictions.\n",
        "- <b><font color='#d55c00'>ONE</font></b> single pdb at the time can be processed by the pipeline. \n",
        "- A  <b><font color='#d55c00'>new run</font></b> can be perform input direcly the new structure in the pdb upload cell and run the prediction cell again\n",
        "\n",
        "****"
      ],
      "metadata": {
        "id": "z2NZN9xv_2Gk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b><font color='#009e74'>PIPELINE : PRELIMINARY OPERATIONS </font></b>\n",
        "These cells MUST be run <b><font color='#d55c00'>INDIVIDUALLY AND SEQUENTIALLY</font></b>, and only once at the start of the notebook.\n",
        "****"
      ],
      "metadata": {
        "id": "I0tOqP4RNHbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><font color='#56b4e9'>PRELIMINARY OPERATIONS</font>: Install condacolab\n",
        "#@markdown Run this cell to install condacolab. After running this cell the kernel will be automatically restarted (wait ~1min before run the next cell)\n",
        "\n",
        "#@markdown **N.B: This cell should be run only ONCE at the START of the notebook.**\n",
        "try:\n",
        "    import google.colab\n",
        "    !pip install condacolab\n",
        "    import condacolab\n",
        "    condacolab.install()\n",
        "except ModuleNotFoundError:\n",
        "    pass"
      ],
      "metadata": {
        "cellView": "form",
        "id": "40mpLhXNEekO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><font color='#56b4e9'>PRELIMINARY OPERATIONS</font>: Setup enviroment and dependencies</b>\n",
        "\n",
        "#@markdown Run this cell to install the required enviroment and dependencies (~10 minutes)\n",
        "\n",
        "#@markdown **N.B: This cell should be run only ONCE at the START of the notebook.**\n",
        "! rm -r sample_data\n",
        "\n",
        "# install dependencies present in pip\n",
        "! pip install numpy==1.17.3 torch==1.7.1 biopython==1.72 matplotlib pdb-tools &> /dev/null\n",
        "! pip install --upgrade pdb-tools\n",
        "! echo \"-> packages with pip installed!\"\n",
        "\n",
        "! mamba install  mpl-scatter-density ptitprince pdbfixer openmm=7.5.1 pandas=1.4.4 -c omnia -c conda-forge -c anaconda -c defaults --yes &> /dev/null\n",
        "! echo \"-> packages with conda installed!\"\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "W-rbZalwSFkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><font color='#56b4e9'>PRELIMINARY OPERATIONS</font>: Retrieve parameters and RaSP files</b>\n",
        "\n",
        "#@markdown Run this cell to import RaSP files and parameters\n",
        "\n",
        "#@markdown **N.B: This cell should be run only ONCE at the START of the notebook.**\n",
        "%%bash\n",
        "\n",
        "#install svn\n",
        "apt-get install -qq subversion &> /dev/null\n",
        "\n",
        "#mkdir of necessary folders\n",
        "mkdir data\n",
        "mkdir data/test\n",
        "mkdir data/test/predictions\n",
        "mkdir data/test/predictions/raw\n",
        "mkdir data/test/predictions/cleaned\n",
        "mkdir data/test/predictions/parsed\n",
        "mkdir output/\n",
        "mkdir output/predictions\n",
        "\n",
        "#download project folders from github\n",
        "\n",
        "svn checkout https://github.com/KULL-Centre/papers/trunk/2022/ML-ddG-Blaabjerg-et-al/src  &> /dev/null\n",
        "svn checkout https://github.com/KULL-Centre/papers/trunk/2022/ML-ddG-Blaabjerg-et-al/output/cavity_models  &> /dev/null\n",
        "svn checkout https://github.com/KULL-Centre/papers/trunk/2022/ML-ddG-Blaabjerg-et-al/output/ds_models  &> /dev/null\n",
        "\n",
        "wget -cq https://github.com/KULL-Centre/papers/raw/papers/2022/ML-ddG-Blaabjerg-et-al/data/pdb_frequencies.npz -o /content/data/pdb_frequencies.npz\n",
        "wget -cq https://github.com/KULL-Centre/papers/raw/main/2022/ML-ddG-Blaabjerg-et-al/colab_additonals/colab_additional.zip\n",
        "\n",
        "#extra files for runnin the notebooks\n",
        "\n",
        "mv ds_models ./output/\n",
        "mv cavity_models ./output/\n",
        "\n",
        "unzip colab_additional.zip &> /dev/null\n",
        "rm colab_additional.zip\n",
        "\n",
        "mv /content/colab_additional/best_model_path.txt /content/output/cavity_models/\n",
        "mv /content/colab_additional/clean_pdb.py /content/src/pdb_parser_scripts/\n",
        "mv /content/colab_additional/helpers.py /content/src/\n",
        "mv /content/colab_additional/pdb_frequencies.npz /content/data/\n",
        "\n",
        "echo \"---> Github data imported!\"\n",
        "\n",
        "#get and compile reduce\n",
        "\n",
        "cd src/pdb_parser_scripts\n",
        "git clone https://github.com/rlabduke/reduce.git\n",
        "cd reduce/\n",
        "make &> /dev/null\n",
        "\n",
        "mv /content/colab_additional/reduce /content/src/pdb_parser_scripts/reduce/\n",
        "\n",
        "chmod +x /content/src/pdb_parser_scripts/reduce/reduce \n",
        "echo \"----> reduce installed\"\n",
        "\n",
        "rm -r /content/colab_additional\n",
        "#@markdown ****"
      ],
      "metadata": {
        "id": "NTRMe6SmnAW0",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><font color='#56b4e9'>PRELIMINARY OPERATIONS</font>: Import python libraries, functions and setup common variables</b>\n",
        "\n",
        "#@markdown Run this cell to import libraries and functions necessary for the pipeline.\n",
        "\n",
        "#@markdown **N.B: This cell should be run only ONCE at the START of the notebook.**\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"/usr/local/lib/python3.7/site-packages\")\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import pathlib\n",
        "import subprocess\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import time\n",
        "import datetime\n",
        "import matplotlib\n",
        "from pdbfixer import PDBFixer\n",
        "from simtk.openmm.app import PDBFile\n",
        "from Bio.PDB.Polypeptide import index_to_one, one_to_index\n",
        "from collections import OrderedDict\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from google.colab import files\n",
        "\n",
        "sys.path.append('./src/')\n",
        "\n",
        "from cavity_model import (\n",
        "     CavityModel,\n",
        "     DownstreamModel,\n",
        "     ResidueEnvironment,\n",
        "     ResidueEnvironmentsDataset,\n",
        ")\n",
        "\n",
        "from helpers import (\n",
        "     populate_dfs_with_resenvs,\n",
        "     remove_disulfides,\n",
        "     fermi_transform,\n",
        "     inverse_fermi_transform,\n",
        "     init_lin_weights,\n",
        "     ds_pred,\n",
        "     cavity_to_prism,\n",
        "     get_seq_from_variant,\n",
        ")\n",
        "\n",
        "from visualization import (\n",
        "     hist_plot,\n",
        ")\n",
        "\n",
        "#Extra function to fix pdb\n",
        "\n",
        "# Setup pipeline parameters\n",
        "## Set seeds\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "## Main deep parameters\n",
        "DEVICE = \"cuda\"  # \"cpu\" or \"cuda\"\n",
        "NUM_ENSEMBLE = 10\n",
        "TASK_ID = int(1)\n",
        "PER_TASK = int(1)\n",
        "\n",
        "#@markdown ****"
      ],
      "metadata": {
        "id": "QKz_yq3VxDCR",
        "cellView": "form"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b><font color='#009e74'>PIPELINE : PREDICTIONS </font></b>"
      ],
      "metadata": {
        "id": "OpUMk4UezkLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><font color='#56b4e9'> PDB upload</font></b>\n",
        "\n",
        "#@markdown Choose between <b><font color='#d55c00'> ONE</font></b> of the possible input sources for the target pdb and <b><font color='#d55c00'>leave the other cells empty or unmarked</font></b>\n",
        "#@markdown - AlphaFold2 PDB (v4) via Uniprot ID:\n",
        "AF_ID ='P68871'#@param {type:\"string\"}\n",
        "#@markdown - PDB ID (imported from RCSB PDB):\n",
        "PDB_ID =''#@param {type:\"string\"}\n",
        "#@markdown - Upload custom PDB\n",
        "PDB_custom =False#@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown\n",
        "\n",
        "#@markdown Select target chain (default A)\n",
        "chain='A' #@param {type:'string'}\n",
        "\n",
        "if os.path.exists(\"/content/query_protein.pdb\"):\n",
        "    os.remove(\"/content/query_protein.pdb\")\n",
        "if os.path.exists(\"/content/data/test/predictions/raw/query_protein_uniquechain.pdb\"):\n",
        "    os.remove(\"/content/data/test/predictions/raw/query_protein_uniquechain.pdb\")\n",
        "if os.path.exists(\"/content/data/test/predictions/cleaned/query_protein_uniquechain_clean.pdb\"):\n",
        "    os.remove(\"/content/data/test/predictions/cleaned/query_protein_uniquechain_clean.pdb\")\n",
        "if os.path.exists(\"/content/data/test/predictions/parsed/query_protein_uniquechain_clean_coordinate_features.npz\"):\n",
        "    os.remove(\"/content/data/test/predictions/parsed/query_protein_uniquechain_clean_coordinate_features.npz\")\n",
        "\n",
        "if PDB_custom:\n",
        "  print('Upload PDB file:')\n",
        "  uploaded_pdb = files.upload()\n",
        "  for fn in uploaded_pdb.keys():\n",
        "    os.rename(fn, f\"/content/query_protein.pdb\")\n",
        "    print('PDB file correctly loaded')\n",
        "elif (AF_ID !='') and (len(AF_ID)>=6) : \n",
        "    subprocess.call(['curl','-s','-f',f'https://alphafold.ebi.ac.uk/files/AF-{AF_ID}-F1-model_v4.pdb','-o','/content/query_protein.pdb'])\n",
        "elif (PDB_ID !='') and (len(PDB_ID)==4):\n",
        "    subprocess.call(['curl','-s','-f',f'https://files.rcsb.org/download/{PDB_ID}.pdb','-o','/content/query_protein.pdb'])\n",
        "\n",
        "else:\n",
        "  print(f'ERROR: any PDB uploaded, please select one of the above inputs')\n",
        "\n",
        "#@markdown N.B. This cell will also perform preliminary operations to correcly format the uploaded PDB\n",
        "\n",
        "## remove other chains and move to raw folder\n",
        "!pdb_selchain -\"$chain\" /content/query_protein.pdb | pdb_delhetatm | pdb_delres --999:0:1 | pdb_fixinsert | pdb_tidy  > /content/data/test/predictions/raw/query_protein_uniquechain.pdb\n",
        "# Select PDBs to run during this task - could be simplified if we decide to set PER_TASK = 1 for all cases\n",
        "\n",
        "pdb_input_dir = \"data/test/predictions/raw/\"\n",
        "input_pdbs = sorted(list(filter(lambda x: x.endswith(\".pdb\"), os.listdir('data/test/predictions/raw/'))))\n",
        "start = (TASK_ID-1)*(PER_TASK)\n",
        "end = (TASK_ID*PER_TASK)\n",
        "if end > len(input_pdbs):\n",
        "    end = len(input_pdbs) #avoid end index exceeding length of list\n",
        "pdbs = input_pdbs[start:end] \n",
        "pdb_names = [i.split(\".\")[0] for i in pdbs]\n",
        "print(pdb_names)\n",
        "print(f\"Pre-processing PDBs ...\")\n",
        "\n",
        "!python3 /content/src/pdb_parser_scripts/clean_pdb.py --pdb_file_in /content/data/test/predictions/raw/query_protein_uniquechain.pdb --out_dir /content/data/test/predictions/cleaned/ --reduce_exe /content/src/pdb_parser_scripts/reduce/reduce #&> /dev/null\n",
        "!python3 /content/src/pdb_parser_scripts/extract_environments.py --pdb_in /content/data/test/predictions/cleaned/query_protein_uniquechain_clean.pdb  --out_dir /content/data/test/predictions/parsed/  #&> /dev/null\n",
        "\n",
        "if os.path.exists(\"/content/data/test/predictions/parsed/query_protein_uniquechain_clean_coordinate_features.npz\"):\n",
        "  print(f\"Pre-processing PDBs correctly ended\")\n",
        "else:\n",
        "  print(f\"Pre-processing PDB didn't end correcly, please check input informations\")\n",
        "\n",
        "#@markdown ****"
      ],
      "metadata": {
        "id": "Z8nUmHI5rgjy",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title <b><font color='#56b4e9'> Pipeline RUN </font></b>\n",
        "\n",
        "#@markdown <b><font color='#d55c00'>Execute the cell</font></b> to run the pipeline and generate **saturation mutagenesis predictions of thermodynamic stability changes** predictions\n",
        "\n",
        "### Pre-process structure data\n",
        "\n",
        "# Create temporary residue environment datasets to more easily match ddG data\n",
        "pdb_filenames_ds = sorted(glob.glob(f\"/content/data/test/predictions/parsed/*coord*\"))\n",
        "\n",
        "dataset_structure = ResidueEnvironmentsDataset(pdb_filenames_ds, transformer=None)\n",
        "\n",
        "resenv_dataset = {}\n",
        "for resenv in dataset_structure:\n",
        "    if AF_ID!='':\n",
        "      key = (f\"--{AF_ID}--{resenv.chain_id}--{resenv.pdb_residue_number}--{index_to_one(resenv.restype_index)}--\"\n",
        "          )\n",
        "    elif PDB_ID!='':\n",
        "      key = (f\"--{PDB_ID}--{resenv.chain_id}--{resenv.pdb_residue_number}--{index_to_one(resenv.restype_index)}--\"\n",
        "          )\n",
        "    else:\n",
        "      key = (f\"--{'CUSTOM'}--{resenv.chain_id}--{resenv.pdb_residue_number}--{index_to_one(resenv.restype_index)}--\"\n",
        "          )\n",
        "    resenv_dataset[key] = resenv\n",
        "df_structure_no_mt = pd.DataFrame.from_dict(resenv_dataset, orient='index', columns=[\"resenv\"])\n",
        "df_structure_no_mt.reset_index(inplace=True)\n",
        "df_structure_no_mt[\"index\"]=df_structure_no_mt[\"index\"].astype(str)\n",
        "res_info = pd.DataFrame(df_structure_no_mt[\"index\"].str.split('--').tolist(),\n",
        "                        columns = ['blank','pdb_id','chain_id','pos','wt_AA', 'blank2'])\n",
        "\n",
        "df_structure_no_mt[\"pdbid\"] = res_info['pdb_id']\n",
        "df_structure_no_mt[\"chainid\"] = res_info['chain_id']\n",
        "df_structure_no_mt[\"variant\"] = res_info[\"wt_AA\"] + res_info['pos'] + \"X\"\n",
        "aa_list = [\"A\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \n",
        "            \"M\", \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"W\", \"Y\"]\n",
        "df_structure = pd.DataFrame(df_structure_no_mt.values.repeat(20, axis=0), columns=df_structure_no_mt.columns)\n",
        "for i in range(0, len(df_structure), 20):\n",
        "    for j in range(20):\n",
        "        df_structure.iloc[i+j, :][\"variant\"] = df_structure.iloc[i+j, :][\"variant\"][:-1] + aa_list[j]\n",
        "df_structure.drop(columns=\"index\", inplace=True)\n",
        "\n",
        "# Load PDB amino acid frequencies used to approximate unfolded states\n",
        "pdb_nlfs = -np.log(np.load(f\"{os.getcwd()}/data/pdb_frequencies.npz\")[\"frequencies\"])\n",
        "\n",
        "# # Add wt and mt idxs and freqs to df\n",
        "\n",
        "df_structure[\"wt_idx\"] = df_structure.apply(lambda row: one_to_index(row[\"variant\"][0]), axis=1)\n",
        "df_structure[\"mt_idx\"] = df_structure.apply(lambda row: one_to_index(row[\"variant\"][-1]), axis=1)\n",
        "df_structure[\"wt_nlf\"] = df_structure.apply(lambda row: pdb_nlfs[row[\"wt_idx\"]], axis=1)\n",
        "df_structure[\"mt_nlf\"] = df_structure.apply(lambda row: pdb_nlfs[row[\"mt_idx\"]], axis=1)\n",
        "\n",
        "# Define models\n",
        "best_cavity_model_path = open(f\"/content/output/cavity_models/best_model_path.txt\", \"r\").read()\n",
        "cavity_model_net = CavityModel(get_latent=True).to(DEVICE)\n",
        "cavity_model_net.load_state_dict(torch.load(f\"{best_cavity_model_path}\"))\n",
        "cavity_model_net.eval()\n",
        "ds_model_net = DownstreamModel().to(DEVICE)\n",
        "ds_model_net.apply(init_lin_weights)\n",
        "ds_model_net.eval()\n",
        "\n",
        "###set start time\n",
        "start_time = time.perf_counter()\n",
        "\n",
        "# Make ML predictions\n",
        "print(f\"Starting downstream model prediction\")\n",
        "dataset_key=\"predictions\"\n",
        "df_ml = ds_pred(cavity_model_net,\n",
        "                ds_model_net,\n",
        "                df_structure,\n",
        "                dataset_key,\n",
        "                NUM_ENSEMBLE,\n",
        "                DEVICE,\n",
        "                ) \n",
        "print(f\"Finished downstream model prediction\")\n",
        "end_time = time.perf_counter()\n",
        "elapsed = datetime.timedelta(seconds = end_time - start_time)\n",
        "print(\"Complete - prediction execution took\", elapsed)\n",
        "\n",
        "elapsed = datetime.timedelta(seconds = end_time - start_time)\n",
        "print(\"Generating output files\")\n",
        "#Merge and save data with predictions\n",
        "\n",
        "df_total = df_structure.merge(df_ml, on=['pdbid','chainid','variant'], how='outer')\n",
        "#df_total[\"b_factors\"] = df_total.apply(lambda row: row[\"resenv\"].b_factors, axis=1)\n",
        "df_total = df_total.drop(\"resenv\", 1)\n",
        "print(f\"{len(df_structure)-len(df_ml)} data points dropped when matching total data with ml predictions in: {dataset_key}.\")\n",
        "\n",
        "# Parse output into separate files by pdb, print to PRISM format\n",
        "for pdbid in df_total[\"pdbid\"].unique():\n",
        "    df_pdb = df_total[df_total[\"pdbid\"]==pdbid]\n",
        "    for chainid in df_pdb[\"chainid\"].unique():\n",
        "        pred_outfile = f\"{os.getcwd()}/output/{dataset_key}/cavity_pred_{pdbid}_{chainid}.csv\"\n",
        "        print(f\"Parsing predictions from pdb: {pdbid}{chainid} into {pred_outfile}\")\n",
        "        df_chain = df_pdb[df_pdb[\"chainid\"]==chainid]\n",
        "        df_chain = df_chain.assign(pos = df_chain[\"variant\"].str[1:-1])\n",
        "        df_chain['pos'] = pd.to_numeric(df_chain['pos'])\n",
        "        first_res_no = min(df_chain[\"pos\"])\n",
        "        df_chain = df_chain.assign(wt_AA = df_chain[\"variant\"].str[0])\n",
        "        df_chain = df_chain.assign(mt_AA = df_chain[\"variant\"].str[-1])\n",
        "        seq = get_seq_from_variant(df_chain)\n",
        "        df_chain.to_csv(pred_outfile, index=False)\n",
        "        prism_outfile = f\"/content/output/{dataset_key}/prism_cavity_{pdbid}_{chainid}.txt\"\n",
        "\n",
        "        # if (AF_ID !=''):\n",
        "        #   prism_outfile = f\"/content/output/{dataset_key}/prism_cavity_{AF_ID}_{chainid}.txt\"\n",
        "        # elif (PDB_ID !=''):\n",
        "        #   prism_outfile = f\"/content/output/{dataset_key}/prism_cavity_{PDB_ID}_{chainid}.txt\"\n",
        "        # elif PDB_custom:\n",
        "        #   prism_outfile = f\"/content/output/{dataset_key}/prism_cavity_XXXX_{chainid}.txt\"\n",
        "        cavity_to_prism(df_chain, pdbid, chainid, seq, prism_outfile)\n",
        "\n",
        "# End timer and print result\n",
        "#!rm /content/output/predictions/*xxxx*.csv\n",
        "elapsed = datetime.timedelta(seconds = end_time - start_time)\n",
        "print(\"Complete - files generated\")\n",
        "\n",
        "#@markdown ****"
      ],
      "metadata": {
        "id": "tAxW8XNqxCS_",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><font color='#56b4e9'> Download results as archive </font></b>\n",
        "\n",
        "#@markdown Run the cell to <b><font color='#009e74'> download a .zip archive </font></b> with prediction files for the <ins>current run</ins>.\n",
        "\n",
        "#@markdown <ins>Tick</ins> the next box if you ran multiple predictions and you want to <ins>download all of them</ins>.\n",
        "\n",
        "download_all_predictions= False #@param {type:\"boolean\"}\n",
        "\n",
        "if download_all_predictions:\n",
        "  os.system( \"zip -r {} {}\".format( f\"predictions_output_all.zip\" , f\"/content/output/predictions/*\" ) )\n",
        "  files.download(f\"predictions_output_all.zip\")\n",
        "else:\n",
        "  if (AF_ID !=''):\n",
        "    os.system( \"zip -r {} {}\".format( f\"predictions_output_{AF_ID}.zip\" , f\"/content/output/predictions/*{AF_ID}*\" ) )\n",
        "    files.download(f\"predictions_output_{AF_ID}.zip\")\n",
        "  elif (PDB_ID !=''):\n",
        "    os.system( \"zip -r {} {}\".format( f\"predictions_output_{PDB_ID}.zip\" , f\"/content/output/predictions/*{PDB_ID}*\" ) )\n",
        "    files.download(f\"predictions_output_{PDB_ID}.zip\")\n",
        "  else:\n",
        "    os.system( \"zip -r {} {}\".format( f\"predictions_output.zip\" , f\"/content/output/predictions\" ) )\n",
        "    files.download(f\"predictions_output.zip\")\n",
        "\n",
        "  if download_all_predictions:\n",
        "    os.system( \"zip -r {} {}\".format( f\"predictions_output.zip\" , f\"/content/output/predictions\" ) )\n",
        "    files.download(f\"predictions_output_all.zip\")\n",
        "\n",
        "#@markdown **P.S.: prediction files are also stored in the colab file system folder: `/output/predictions/`**\n",
        "#@markdown ****"
      ],
      "metadata": {
        "id": "tVAoMFcNlasI",
        "collapsed": true,
        "cellView": "form",
        "outputId": "d789a132-70b9-481f-fd69-f04043b5e3f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3c257781-2f8b-4ded-8445-90dc689750c9\", \"predictions_output_A0A010IS17.zip\", 219957)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CHANGING LOG**\n",
        "\n",
        "- 12th April 2023: AlphaFold2 database version update to v4.\n",
        "- 3rd April 2023: Torch version and packagess updated for python 3.9\n",
        "\n",
        "\n",
        "\n",
        "**Troubleshooting**\n",
        "\n",
        "- Check that the runtime type is set to GPU at \"Runtime\" -> \"Change runtime type\".\n",
        "- Try to restart the session \"Runtime\" -> \"Factory reset runtime\".\n",
        "- Run the PRELIMINARY OPERATION one at the time, to avoid crashes.\n",
        "- Check your input pdb.\n",
        "\n",
        "\\\\\n",
        "\n",
        "**Known problems:**\n",
        "\n",
        "- Condacolab need to restart the notebook kernel, so preliminart cells MUST be run one at the time to allow this.\n",
        "- Residues with numeration index below 0 are not supported by the output file parser and thus they deleted from the pdb in the pre-processing step.\n",
        "- Insertion annotations in the pdb are not supported. Any annotations is actually deleted during the pre-processing step.\n",
        "\n",
        "\\\\\n",
        "\n",
        "**License:**\n",
        "\n",
        "RaSP's source code is licensed under the permissive Apache Licence, Version 2.0.\n",
        " Additionally, this notebook uses the reduce source code which license could be find in `/content/src/pdb_parser_scripts/reduce/`\n",
        "\n",
        "\\\\\n",
        "\n",
        "**Bugs:**\n",
        "\n",
        "For any bugs please report the issue on the project [Github](https://github.com/KULL-Centre/papers/tree/main/2022/ML-ddG-Blaabjerg-et-al) or contact one of the listed authors in the connected [manuscript](https://www.biorxiv.org/content/10.1101/2022.07.14.500157v1).\n",
        "\n",
        "\\\\\n",
        "\n",
        "**Citing this work:**\n",
        "\n",
        "If you use our model please cite:\n",
        "\n",
        "Blaabjerg, L.M., Kassem, M.M., Good, L.L., Jonsson, N., Cagiada, M., Johansson, K.E., Boomsma, W., Stein, A. and Lindorff-Larsen, K., 2022. *Rapid protein stability prediction using deep learning representations*, bioRxiv. (https://doi.org/10.1101/2022.07.14.500157)\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "@article{blaabjerg2022rapid,\n",
        "  title={Rapid protein stability prediction using deep learning representations},\n",
        "  author={Blaabjerg, Lasse M and Kassem, Maher M and Good, Lydia L and Jonsson, Nicolas and Cagiada, Matteo and Johansson, Kristoffer E and Boomsma, Wouter and Stein, Amelie and Lindorff-Larsen, Kresten},\n",
        "  journal={bioRxiv},\n",
        "  year={2022},\n",
        "  publisher={Cold Spring Harbor Laboratory}\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "1fb2HFDWC2Yu"
      }
    }
  ]
}